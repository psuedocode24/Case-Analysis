{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "b2d44523",
   "metadata": {
    "toc": true
   },
   "source": [
    "<h1>Index<span class=\"tocSkip\"></span></h1>\n",
    "<div class=\"toc\"><ul class=\"toc-item\"><li><span><a href=\"#CAPITAL-ONE-DATA-CHALLENGE\" data-toc-modified-id=\"CAPITAL-ONE-DATA-CHALLENGE-1\"><span class=\"toc-item-num\">1&nbsp;&nbsp;</span>CAPITAL ONE DATA CHALLENGE</a></span><ul class=\"toc-item\"><li><span><a href=\"#Problem-Statement\" data-toc-modified-id=\"Problem-Statement-1.1\"><span class=\"toc-item-num\">1.1&nbsp;&nbsp;</span>Problem Statement</a></span></li><li><span><a href=\"#Data-Sources\" data-toc-modified-id=\"Data-Sources-1.2\"><span class=\"toc-item-num\">1.2&nbsp;&nbsp;</span>Data Sources</a></span></li><li><span><a href=\"#Assumptions\" data-toc-modified-id=\"Assumptions-1.3\"><span class=\"toc-item-num\">1.3&nbsp;&nbsp;</span>Assumptions</a></span></li><li><span><a href=\"#Approach\" data-toc-modified-id=\"Approach-1.4\"><span class=\"toc-item-num\">1.4&nbsp;&nbsp;</span>Approach</a></span></li><li><span><a href=\"#Data-Quality-Check\" data-toc-modified-id=\"Data-Quality-Check-1.5\"><span class=\"toc-item-num\">1.5&nbsp;&nbsp;</span>Data Quality Check</a></span><ul class=\"toc-item\"><li><span><a href=\"#Environment-Setup\" data-toc-modified-id=\"Environment-Setup-1.5.1\"><span class=\"toc-item-num\">1.5.1&nbsp;&nbsp;</span>Environment Setup</a></span></li><li><span><a href=\"#Data-Understanding\" data-toc-modified-id=\"Data-Understanding-1.5.2\"><span class=\"toc-item-num\">1.5.2&nbsp;&nbsp;</span>Data Understanding</a></span><ul class=\"toc-item\"><li><span><a href=\"#1.-Accuracy\" data-toc-modified-id=\"1.-Accuracy-1.5.2.1\"><span class=\"toc-item-num\">1.5.2.1&nbsp;&nbsp;</span>1. Accuracy</a></span></li><li><span><a href=\"#2.-Completeness\" data-toc-modified-id=\"2.-Completeness-1.5.2.2\"><span class=\"toc-item-num\">1.5.2.2&nbsp;&nbsp;</span>2. Completeness</a></span></li></ul></li></ul></li><li><span><a href=\"#Exploring-the-Airport-Codes-data\" data-toc-modified-id=\"Exploring-the-Airport-Codes-data-1.6\"><span class=\"toc-item-num\">1.6&nbsp;&nbsp;</span>Exploring the Airport Codes data</a></span><ul class=\"toc-item\"><li><ul class=\"toc-item\"><li><span><a href=\"#Size-of-the-data\" data-toc-modified-id=\"Size-of-the-data-1.6.0.1\"><span class=\"toc-item-num\">1.6.0.1&nbsp;&nbsp;</span>Size of the data</a></span></li><li><span><a href=\"#Sample-data\" data-toc-modified-id=\"Sample-data-1.6.0.2\"><span class=\"toc-item-num\">1.6.0.2&nbsp;&nbsp;</span>Sample data</a></span></li><li><span><a href=\"#Checking-the-data-types-of-all-columns\" data-toc-modified-id=\"Checking-the-data-types-of-all-columns-1.6.0.3\"><span class=\"toc-item-num\">1.6.0.3&nbsp;&nbsp;</span>Checking the data types of all columns</a></span></li><li><span><a href=\"#Checking-the-null-counts-for-all-columns\" data-toc-modified-id=\"Checking-the-null-counts-for-all-columns-1.6.0.4\"><span class=\"toc-item-num\">1.6.0.4&nbsp;&nbsp;</span>Checking the null counts for all columns</a></span></li><li><span><a href=\"#Plotting-the-missing-values-in-the-Airports-Dataset\" data-toc-modified-id=\"Plotting-the-missing-values-in-the-Airports-Dataset-1.6.0.5\"><span class=\"toc-item-num\">1.6.0.5&nbsp;&nbsp;</span>Plotting the missing values in the Airports Dataset</a></span></li><li><span><a href=\"#Filtering-for-Medium-and-Large-airports\" data-toc-modified-id=\"Filtering-for-Medium-and-Large-airports-1.6.0.6\"><span class=\"toc-item-num\">1.6.0.6&nbsp;&nbsp;</span>Filtering for Medium and Large airports</a></span></li><li><span><a href=\"#Plotting-the-Airport-Types\" data-toc-modified-id=\"Plotting-the-Airport-Types-1.6.0.7\"><span class=\"toc-item-num\">1.6.0.7&nbsp;&nbsp;</span>Plotting the Airport Types</a></span></li><li><span><a href=\"#Examining-the-NULL-value-counts-again-for-the-subset-of-the-data\" data-toc-modified-id=\"Examining-the-NULL-value-counts-again-for-the-subset-of-the-data-1.6.0.8\"><span class=\"toc-item-num\">1.6.0.8&nbsp;&nbsp;</span>Examining the NULL value counts again for the subset of the data</a></span></li><li><span><a href=\"#Filtering-for-US-based-airports\" data-toc-modified-id=\"Filtering-for-US-based-airports-1.6.0.9\"><span class=\"toc-item-num\">1.6.0.9&nbsp;&nbsp;</span>Filtering for US based airports</a></span></li><li><span><a href=\"#Size-of-the-subset\" data-toc-modified-id=\"Size-of-the-subset-1.6.0.10\"><span class=\"toc-item-num\">1.6.0.10&nbsp;&nbsp;</span>Size of the subset</a></span></li><li><span><a href=\"#Checking-for-NULL-IATA-codes\" data-toc-modified-id=\"Checking-for-NULL-IATA-codes-1.6.0.11\"><span class=\"toc-item-num\">1.6.0.11&nbsp;&nbsp;</span>Checking for NULL IATA codes</a></span></li><li><span><a href=\"#We-require-IATA-codes-for-joining-the-data-with-the-ticket-and-the-flight-dataset.-Hence-we-will-drop-the-37-rows-with-NULL-IATA-codes\" data-toc-modified-id=\"We-require-IATA-codes-for-joining-the-data-with-the-ticket-and-the-flight-dataset.-Hence-we-will-drop-the-37-rows-with-NULL-IATA-codes-1.6.0.12\"><span class=\"toc-item-num\">1.6.0.12&nbsp;&nbsp;</span><strong>We require IATA codes for joining the data with the ticket and the flight dataset. Hence we will drop the 37 rows with NULL IATA codes</strong></a></span></li></ul></li></ul></li><li><span><a href=\"#Exploring-the-Tickets-data\" data-toc-modified-id=\"Exploring-the-Tickets-data-1.7\"><span class=\"toc-item-num\">1.7&nbsp;&nbsp;</span>Exploring the Tickets data</a></span><ul class=\"toc-item\"><li><ul class=\"toc-item\"><li><span><a href=\"#Checking-the-dtypes-and-Missing-Value-counts\" data-toc-modified-id=\"Checking-the-dtypes-and-Missing-Value-counts-1.7.0.1\"><span class=\"toc-item-num\">1.7.0.1&nbsp;&nbsp;</span>Checking the dtypes and Missing Value counts</a></span></li><li><span><a href=\"#Filtering-the-data-for-round-trips\" data-toc-modified-id=\"Filtering-the-data-for-round-trips-1.7.0.2\"><span class=\"toc-item-num\">1.7.0.2&nbsp;&nbsp;</span>Filtering the data for round trips</a></span></li><li><span><a href=\"#Filtering-the-tickets-data-for-US-Domestic-Flights\" data-toc-modified-id=\"Filtering-the-tickets-data-for-US-Domestic-Flights-1.7.0.3\"><span class=\"toc-item-num\">1.7.0.3&nbsp;&nbsp;</span>Filtering the tickets data for US Domestic Flights</a></span></li><li><span><a href=\"#Checking-for-missing-values\" data-toc-modified-id=\"Checking-for-missing-values-1.7.0.4\"><span class=\"toc-item-num\">1.7.0.4&nbsp;&nbsp;</span>Checking for missing values</a></span></li><li><span><a href=\"#Data-Quality-Issue\" data-toc-modified-id=\"Data-Quality-Issue-1.7.0.5\"><span class=\"toc-item-num\">1.7.0.5&nbsp;&nbsp;</span>Data Quality Issue</a></span></li><li><span><a href=\"#Defining-a-new-column-'TRIP'-that-corresponds-to-the-round-trip-between-ORIGIN-and-the-DESTINATION-column\" data-toc-modified-id=\"Defining-a-new-column-'TRIP'-that-corresponds-to-the-round-trip-between-ORIGIN-and-the-DESTINATION-column-1.7.0.6\"><span class=\"toc-item-num\">1.7.0.6&nbsp;&nbsp;</span>Defining a new column 'TRIP' that corresponds to the round trip between ORIGIN and the DESTINATION column</a></span></li><li><span><a href=\"#Outlier-Detection\" data-toc-modified-id=\"Outlier-Detection-1.7.0.7\"><span class=\"toc-item-num\">1.7.0.7&nbsp;&nbsp;</span>Outlier Detection</a></span></li><li><span><a href=\"#Removing-the-outliers-in-the-tickets-data\" data-toc-modified-id=\"Removing-the-outliers-in-the-tickets-data-1.7.0.8\"><span class=\"toc-item-num\">1.7.0.8&nbsp;&nbsp;</span>Removing the outliers in the tickets data</a></span></li><li><span><a href=\"#We-can-see-that-the-fares-on-average-cost-~500-USD.-With-most-of-the-airlines-charging-roughly-the-same-amount-for-a-roundtrip-except-the-airlines-that-have-either-monopoly-operating-in-a-particular-region-or-are-cost-effecient-like-Frontier-Airlines(F9),-Spirit-Airlines(NK),-Compass-Airlines(CP),-Horizon-Air(QX),-Southwest-Airlines(WN),-Sky-West-Airlines(SY),-Hawaiian-Airlines(HA)-and-Allegiant-Air(G4).\" data-toc-modified-id=\"We-can-see-that-the-fares-on-average-cost-~500-USD.-With-most-of-the-airlines-charging-roughly-the-same-amount-for-a-roundtrip-except-the-airlines-that-have-either-monopoly-operating-in-a-particular-region-or-are-cost-effecient-like-Frontier-Airlines(F9),-Spirit-Airlines(NK),-Compass-Airlines(CP),-Horizon-Air(QX),-Southwest-Airlines(WN),-Sky-West-Airlines(SY),-Hawaiian-Airlines(HA)-and-Allegiant-Air(G4).-1.7.0.9\"><span class=\"toc-item-num\">1.7.0.9&nbsp;&nbsp;</span>We can see that the fares on average cost ~500 USD. With most of the airlines charging roughly the same amount for a roundtrip except the airlines that have either monopoly operating in a particular region or are cost effecient like Frontier Airlines(F9), Spirit Airlines(NK), Compass Airlines(CP), Horizon Air(QX), Southwest Airlines(WN), Sky West Airlines(SY), Hawaiian Airlines(HA) and Allegiant Air(G4).</a></span></li></ul></li></ul></li><li><span><a href=\"#Exploring-the-Flights-Data\" data-toc-modified-id=\"Exploring-the-Flights-Data-1.8\"><span class=\"toc-item-num\">1.8&nbsp;&nbsp;</span>Exploring the Flights Data</a></span><ul class=\"toc-item\"><li><ul class=\"toc-item\"><li><span><a href=\"#Size-of-the-data\" data-toc-modified-id=\"Size-of-the-data-1.8.0.1\"><span class=\"toc-item-num\">1.8.0.1&nbsp;&nbsp;</span>Size of the data</a></span></li><li><span><a href=\"#Sample-of-the-data\" data-toc-modified-id=\"Sample-of-the-data-1.8.0.2\"><span class=\"toc-item-num\">1.8.0.2&nbsp;&nbsp;</span>Sample of the data</a></span></li><li><span><a href=\"#Checking-the-data-types-and-missing-value-counts\" data-toc-modified-id=\"Checking-the-data-types-and-missing-value-counts-1.8.0.3\"><span class=\"toc-item-num\">1.8.0.3&nbsp;&nbsp;</span>Checking the data types and missing value counts</a></span></li><li><span><a href=\"#Data-Quality-Issue:\" data-toc-modified-id=\"Data-Quality-Issue:-1.8.0.4\"><span class=\"toc-item-num\">1.8.0.4&nbsp;&nbsp;</span>Data Quality Issue:</a></span></li><li><span><a href=\"#Checking-the-distribution-of-cancelled-flights\" data-toc-modified-id=\"Checking-the-distribution-of-cancelled-flights-1.8.0.5\"><span class=\"toc-item-num\">1.8.0.5&nbsp;&nbsp;</span>Checking the distribution of cancelled flights</a></span></li><li><span><a href=\"#Filtering-the-CANCELLED-flights\" data-toc-modified-id=\"Filtering-the-CANCELLED-flights-1.8.0.6\"><span class=\"toc-item-num\">1.8.0.6&nbsp;&nbsp;</span>Filtering the CANCELLED flights</a></span></li><li><span><a href=\"#FILTERING-the-ORIGIN-and-DESTINATION-and-MERGING-the-AIRPORT-data\" data-toc-modified-id=\"FILTERING-the-ORIGIN-and-DESTINATION-and-MERGING-the-AIRPORT-data-1.8.0.7\"><span class=\"toc-item-num\">1.8.0.7&nbsp;&nbsp;</span>FILTERING the ORIGIN and DESTINATION and MERGING the AIRPORT data</a></span></li></ul></li><li><span><a href=\"#Pre-processing-and-cleaning-the-flights-data\" data-toc-modified-id=\"Pre-processing-and-cleaning-the-flights-data-1.8.1\"><span class=\"toc-item-num\">1.8.1&nbsp;&nbsp;</span>Pre-processing and cleaning the flights data</a></span><ul class=\"toc-item\"><li><span><a href=\"#Dtype-Correction\" data-toc-modified-id=\"Dtype-Correction-1.8.1.1\"><span class=\"toc-item-num\">1.8.1.1&nbsp;&nbsp;</span>Dtype Correction</a></span></li><li><span><a href=\"#Outlier-Detection\" data-toc-modified-id=\"Outlier-Detection-1.8.1.2\"><span class=\"toc-item-num\">1.8.1.2&nbsp;&nbsp;</span>Outlier Detection</a></span></li><li><span><a href=\"#Removing-outliers\" data-toc-modified-id=\"Removing-outliers-1.8.1.3\"><span class=\"toc-item-num\">1.8.1.3&nbsp;&nbsp;</span>Removing outliers</a></span></li><li><span><a href=\"#Missing-value-treatment\" data-toc-modified-id=\"Missing-value-treatment-1.8.1.4\"><span class=\"toc-item-num\">1.8.1.4&nbsp;&nbsp;</span>Missing value treatment</a></span></li><li><span><a href=\"#Checking-for-missing-values-in-the-aggregare-data\" data-toc-modified-id=\"Checking-for-missing-values-in-the-aggregare-data-1.8.1.5\"><span class=\"toc-item-num\">1.8.1.5&nbsp;&nbsp;</span>Checking for missing values in the aggregare data</a></span></li></ul></li></ul></li></ul></li><li><span><a href=\"#Defining-the-Metrics\" data-toc-modified-id=\"Defining-the-Metrics-2\"><span class=\"toc-item-num\">2&nbsp;&nbsp;</span>Defining the Metrics</a></span><ul class=\"toc-item\"><li><span><a href=\"#Calculating-the-cost\" data-toc-modified-id=\"Calculating-the-cost-2.1\"><span class=\"toc-item-num\">2.1&nbsp;&nbsp;</span>Calculating the cost</a></span><ul class=\"toc-item\"><li><span><a href=\"#Adding-base-costs-to-the-airport-data\" data-toc-modified-id=\"Adding-base-costs-to-the-airport-data-2.1.1\"><span class=\"toc-item-num\">2.1.1&nbsp;&nbsp;</span>Adding base costs to the airport data</a></span></li><li><span><a href=\"#Adding-late-cost-and-misc-cost-to-the-flights-data\" data-toc-modified-id=\"Adding-late-cost-and-misc-cost-to-the-flights-data-2.1.2\"><span class=\"toc-item-num\">2.1.2&nbsp;&nbsp;</span>Adding late cost and misc cost to the flights data</a></span><ul class=\"toc-item\"><li><span><a href=\"#Aggregating-the-flight-data\" data-toc-modified-id=\"Aggregating-the-flight-data-2.1.2.1\"><span class=\"toc-item-num\">2.1.2.1&nbsp;&nbsp;</span>Aggregating the flight data</a></span></li><li><span><a href=\"#Outlier-Detection\" data-toc-modified-id=\"Outlier-Detection-2.1.2.2\"><span class=\"toc-item-num\">2.1.2.2&nbsp;&nbsp;</span>Outlier Detection</a></span></li></ul></li></ul></li><li><span><a href=\"#Revenue-Metrics\" data-toc-modified-id=\"Revenue-Metrics-2.2\"><span class=\"toc-item-num\">2.2&nbsp;&nbsp;</span>Revenue Metrics</a></span><ul class=\"toc-item\"><li><span><a href=\"#Data-Munging\" data-toc-modified-id=\"Data-Munging-2.2.1\"><span class=\"toc-item-num\">2.2.1&nbsp;&nbsp;</span>Data Munging</a></span><ul class=\"toc-item\"><li><span><a href=\"#Outlier-Detection\" data-toc-modified-id=\"Outlier-Detection-2.2.1.1\"><span class=\"toc-item-num\">2.2.1.1&nbsp;&nbsp;</span>Outlier Detection</a></span></li></ul></li></ul></li><li><span><a href=\"#Summarizing-Insights\" data-toc-modified-id=\"Summarizing-Insights-2.3\"><span class=\"toc-item-num\">2.3&nbsp;&nbsp;</span>Summarizing Insights</a></span><ul class=\"toc-item\"><li><ul class=\"toc-item\"><li><span><a href=\"#Checking-the-late-%-for-the-above-subset-of-flights\" data-toc-modified-id=\"Checking-the-late-%-for-the-above-subset-of-flights-2.3.0.1\"><span class=\"toc-item-num\">2.3.0.1&nbsp;&nbsp;</span>Checking the late % for the above subset of flights</a></span></li></ul></li><li><span><a href=\"#Preprocessing-for-Visualization\" data-toc-modified-id=\"Preprocessing-for-Visualization-2.3.1\"><span class=\"toc-item-num\">2.3.1&nbsp;&nbsp;</span>Preprocessing for Visualization</a></span></li><li><span><a href=\"#1.-Top-10-Busiest-round-trips-in-terms-of-number-of-flights-in-a-quarter\" data-toc-modified-id=\"1.-Top-10-Busiest-round-trips-in-terms-of-number-of-flights-in-a-quarter-2.3.2\"><span class=\"toc-item-num\">2.3.2&nbsp;&nbsp;</span>1. Top 10 Busiest round trips in terms of number of flights in a quarter</a></span></li><li><span><a href=\"#2.-Top-10-Profitable-round-trips-in-terms-of-total-profit-in-a-quarter\" data-toc-modified-id=\"2.-Top-10-Profitable-round-trips-in-terms-of-total-profit-in-a-quarter-2.3.3\"><span class=\"toc-item-num\">2.3.3&nbsp;&nbsp;</span>2. Top 10 Profitable round trips in terms of total profit in a quarter</a></span><ul class=\"toc-item\"><li><span><a href=\"#Summary-Cols\" data-toc-modified-id=\"Summary-Cols-2.3.3.1\"><span class=\"toc-item-num\">2.3.3.1&nbsp;&nbsp;</span>Summary Cols</a></span></li></ul></li><li><span><a href=\"#3.-Recommendation\" data-toc-modified-id=\"3.-Recommendation-2.3.4\"><span class=\"toc-item-num\">2.3.4&nbsp;&nbsp;</span>3. Recommendation</a></span><ul class=\"toc-item\"><li><span><a href=\"#Recommended-round-trips\" data-toc-modified-id=\"Recommended-round-trips-2.3.4.1\"><span class=\"toc-item-num\">2.3.4.1&nbsp;&nbsp;</span>Recommended round trips</a></span></li></ul></li><li><span><a href=\"#4.-Number-of-flights-to-breakeven\" data-toc-modified-id=\"4.-Number-of-flights-to-breakeven-2.3.5\"><span class=\"toc-item-num\">2.3.5&nbsp;&nbsp;</span>4. Number of flights to breakeven</a></span><ul class=\"toc-item\"><li><span><a href=\"#Time-taken-to-breakeven\" data-toc-modified-id=\"Time-taken-to-breakeven-2.3.5.1\"><span class=\"toc-item-num\">2.3.5.1&nbsp;&nbsp;</span>Time taken to breakeven</a></span></li></ul></li><li><span><a href=\"#5.-KPI's-to-track-for-future\" data-toc-modified-id=\"5.-KPI's-to-track-for-future-2.3.6\"><span class=\"toc-item-num\">2.3.6&nbsp;&nbsp;</span>5. KPI's to track for future</a></span></li><li><span><a href=\"#What's-next?\" data-toc-modified-id=\"What's-next?-2.3.7\"><span class=\"toc-item-num\">2.3.7&nbsp;&nbsp;</span>What's next?</a></span></li></ul></li></ul></li></ul></div>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8621d0c1",
   "metadata": {},
   "source": [
    "# CAPITAL ONE DATA CHALLENGE"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e7dadfc9",
   "metadata": {},
   "source": [
    "****Ashi Malik****\n",
    "\n",
    "03/21/2022"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "41479784",
   "metadata": {},
   "source": [
    "## Problem Statement"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ba4d2f90",
   "metadata": {},
   "source": [
    "An airline firm is looking to enter the US domestic airline market. The company has already decided to begin with 5 round trips between medium and large US airports. \n",
    "\n",
    "Keeping the brand motto in mind \"On Time, for you\" as a consultant we need to build a data product with supportive concnlusions as to which 5 round trips route are best investment options for entering the domestic airline market in US."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "74865abb",
   "metadata": {},
   "source": [
    "## Data Sources"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "aad44fdc",
   "metadata": {},
   "source": [
    "There are three data sources:\n",
    "\n",
    "1.  Tickets dataset provides information on the ticket prices.\n",
    "2.  Airports dataset provides information whether an airport is considered medium or large sized. \n",
    "3.  Flights dataset provdies information the available routes from origin to destination."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "afdcdbc1",
   "metadata": {},
   "source": [
    "## Assumptions"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fd3ad88e",
   "metadata": {},
   "source": [
    "1. All carriers are given equal importance. \n",
    "2. The time value of ticket price per passenger is 0%(i.e $1 today is the same worth as 100 years from now )\n",
    "3. Flights that have monopoly in a specific route is ignored from being considered as a prospective candidate in the best routes for instance Hawian Airlines."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "65d52ae8",
   "metadata": {},
   "source": [
    "## Approach\n",
    "\n",
    "Since we are using data to solve a business problem, let’s keep the following framework in mind while approaching the problem:\n",
    "\n",
    "1. No Hardcoding: The analysis should be reusable for a new prospective route or a different kind of airport type.\n",
    "\n",
    "2. Business Understanding: The goal is to select best flight roundtrip routes to maximize profits and hence, all factors needed to evaluate the same should be critically analyzed.\n",
    "\n",
    "3. Data Understanding: All variables should be clearly understood and only the ones pertinent to the problem should be selected and cleaned for analysis.\n",
    "\n",
    "4. Assumptions: All assumptions in addition to the ones already mentioned should be backed by data/reasoning from external sources or derived from existing assumptions.\n",
    "\n",
    "5. Profitability Metrics: Based on available data, profitability metrics should be defined and visualized to aid decision making."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0e542626",
   "metadata": {},
   "source": [
    "## Data Quality Check"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3dfdc999",
   "metadata": {},
   "source": [
    "### Environment Setup"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0ed089fa",
   "metadata": {},
   "source": [
    "Importing the required packages for our analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "f9009038",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Pandas Version 1.2.4\n",
    "import pandas as pd\n",
    "# Numpy Version 1.20.1\n",
    "import numpy as np\n",
    "# Regex Version as 2.2.1\n",
    "import re\n",
    "# Seaborn Version as 0.11.1\n",
    "import seaborn as sns\n",
    "import warnings\n",
    "# MissingNo Version 0.4.2\n",
    "import missingno as msno\n",
    "warnings.filterwarnings(\"ignore\")\n",
    "import plotly.graph_objects as go\n",
    "\n",
    "from matplotlib import pyplot as plt\n",
    "\n",
    "# Setting maximum number of rows and columns to display in the notebook\n",
    "pd.set_option('display.max_rows', 5000)\n",
    "pd.set_option('display.max_columns', 500)\n",
    "\n",
    "sns.set(rc={'figure.figsize':(15,8)})"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3e631896",
   "metadata": {},
   "source": [
    "### Data Understanding"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1bf874a5",
   "metadata": {},
   "source": [
    "Three important criteria for checking the quality of data in this context are:\n",
    "\n",
    "#### 1. Accuracy\n",
    "\n",
    "The values present in all columns should be accurate.\n",
    "\n",
    "#### 2. Completeness\n",
    "\n",
    "The data should not have any missing values.\n",
    "\n",
    "Before we begin the quality check, let’s ensure we only use data needed for the scope of the problem."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "613564ea",
   "metadata": {},
   "source": [
    "## Exploring the Airport Codes data"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "30945ea5",
   "metadata": {},
   "source": [
    "Airports Codes Dataset: Exploring the dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "b0b9a3c7",
   "metadata": {},
   "outputs": [
    {
     "ename": "FileNotFoundError",
     "evalue": "[Errno 2] No such file or directory: 'Airport_Codes.csv'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mFileNotFoundError\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-8-fa66a1ed82a3>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0;31m# Reading the Airport_Codes.csv file\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 2\u001b[0;31m \u001b[0mairports\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mpd\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mread_csv\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'Airport_Codes.csv'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m/Applications/anaconda3/lib/python3.8/site-packages/pandas/io/parsers.py\u001b[0m in \u001b[0;36mread_csv\u001b[0;34m(filepath_or_buffer, sep, delimiter, header, names, index_col, usecols, squeeze, prefix, mangle_dupe_cols, dtype, engine, converters, true_values, false_values, skipinitialspace, skiprows, skipfooter, nrows, na_values, keep_default_na, na_filter, verbose, skip_blank_lines, parse_dates, infer_datetime_format, keep_date_col, date_parser, dayfirst, cache_dates, iterator, chunksize, compression, thousands, decimal, lineterminator, quotechar, quoting, doublequote, escapechar, comment, encoding, dialect, error_bad_lines, warn_bad_lines, delim_whitespace, low_memory, memory_map, float_precision, storage_options)\u001b[0m\n\u001b[1;32m    608\u001b[0m     \u001b[0mkwds\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mupdate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mkwds_defaults\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    609\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 610\u001b[0;31m     \u001b[0;32mreturn\u001b[0m \u001b[0m_read\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfilepath_or_buffer\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mkwds\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    611\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    612\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/Applications/anaconda3/lib/python3.8/site-packages/pandas/io/parsers.py\u001b[0m in \u001b[0;36m_read\u001b[0;34m(filepath_or_buffer, kwds)\u001b[0m\n\u001b[1;32m    460\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    461\u001b[0m     \u001b[0;31m# Create the parser.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 462\u001b[0;31m     \u001b[0mparser\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mTextFileReader\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfilepath_or_buffer\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwds\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    463\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    464\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mchunksize\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0miterator\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/Applications/anaconda3/lib/python3.8/site-packages/pandas/io/parsers.py\u001b[0m in \u001b[0;36m__init__\u001b[0;34m(self, f, engine, **kwds)\u001b[0m\n\u001b[1;32m    817\u001b[0m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0moptions\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m\"has_index_names\"\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mkwds\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m\"has_index_names\"\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    818\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 819\u001b[0;31m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_engine\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_make_engine\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mengine\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    820\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    821\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mclose\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/Applications/anaconda3/lib/python3.8/site-packages/pandas/io/parsers.py\u001b[0m in \u001b[0;36m_make_engine\u001b[0;34m(self, engine)\u001b[0m\n\u001b[1;32m   1048\u001b[0m             )\n\u001b[1;32m   1049\u001b[0m         \u001b[0;31m# error: Too many arguments for \"ParserBase\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1050\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mmapping\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mengine\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mf\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0moptions\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# type: ignore[call-arg]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1051\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1052\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m_failover_to_python\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/Applications/anaconda3/lib/python3.8/site-packages/pandas/io/parsers.py\u001b[0m in \u001b[0;36m__init__\u001b[0;34m(self, src, **kwds)\u001b[0m\n\u001b[1;32m   1865\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1866\u001b[0m         \u001b[0;31m# open handles\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1867\u001b[0;31m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_open_handles\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msrc\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mkwds\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1868\u001b[0m         \u001b[0;32massert\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mhandles\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1869\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0mkey\u001b[0m \u001b[0;32min\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0;34m\"storage_options\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\"encoding\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\"memory_map\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\"compression\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/Applications/anaconda3/lib/python3.8/site-packages/pandas/io/parsers.py\u001b[0m in \u001b[0;36m_open_handles\u001b[0;34m(self, src, kwds)\u001b[0m\n\u001b[1;32m   1360\u001b[0m         \u001b[0mLet\u001b[0m \u001b[0mthe\u001b[0m \u001b[0mreaders\u001b[0m \u001b[0mopen\u001b[0m \u001b[0mIOHanldes\u001b[0m \u001b[0mafter\u001b[0m \u001b[0mthey\u001b[0m \u001b[0mare\u001b[0m \u001b[0mdone\u001b[0m \u001b[0;32mwith\u001b[0m \u001b[0mtheir\u001b[0m \u001b[0mpotential\u001b[0m \u001b[0mraises\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1361\u001b[0m         \"\"\"\n\u001b[0;32m-> 1362\u001b[0;31m         self.handles = get_handle(\n\u001b[0m\u001b[1;32m   1363\u001b[0m             \u001b[0msrc\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1364\u001b[0m             \u001b[0;34m\"r\"\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/Applications/anaconda3/lib/python3.8/site-packages/pandas/io/common.py\u001b[0m in \u001b[0;36mget_handle\u001b[0;34m(path_or_buf, mode, encoding, compression, memory_map, is_text, errors, storage_options)\u001b[0m\n\u001b[1;32m    640\u001b[0m                 \u001b[0merrors\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m\"replace\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    641\u001b[0m             \u001b[0;31m# Encoding\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 642\u001b[0;31m             handle = open(\n\u001b[0m\u001b[1;32m    643\u001b[0m                 \u001b[0mhandle\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    644\u001b[0m                 \u001b[0mioargs\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmode\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mFileNotFoundError\u001b[0m: [Errno 2] No such file or directory: 'Airport_Codes.csv'"
     ]
    }
   ],
   "source": [
    "# Reading the Airport_Codes.csv file\n",
    "airports = pd.read_csv('Airport_Codes.csv')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fb104564",
   "metadata": {},
   "source": [
    "#### Size of the data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ff820c39",
   "metadata": {},
   "outputs": [],
   "source": [
    "airports.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9744a639",
   "metadata": {},
   "source": [
    "We have data corresponding to 55369 airports assuming each row corresponds to a unique airport and the consistency is maintained."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "336898b7",
   "metadata": {},
   "source": [
    "#### Sample data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3fdaf7bb",
   "metadata": {},
   "outputs": [],
   "source": [
    "airports.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "755fae4e",
   "metadata": {},
   "source": [
    "The airports data consists of five columns as shown in the sample dataset."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "de805fd0",
   "metadata": {},
   "source": [
    "#### Checking the data types of all columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "baa6767f",
   "metadata": {},
   "outputs": [],
   "source": [
    "airports.dtypes"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "99f5552f",
   "metadata": {},
   "source": [
    "The data types of all the columns are rightly aligned and need no type conversion."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6488a6c5",
   "metadata": {},
   "source": [
    "#### Checking the null counts for all columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d1424e18",
   "metadata": {},
   "outputs": [],
   "source": [
    "airports.isna().sum()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8371b509",
   "metadata": {},
   "source": [
    "#### Plotting the missing values in the Airports Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "187101fa",
   "metadata": {},
   "outputs": [],
   "source": [
    "msno.bar(airports)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5d02ec36",
   "metadata": {},
   "source": [
    "We can see that the data is not clean since there are missing values for the continent to which the airport belongs and the IATA code for the corresponding airport"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4ec1ae17",
   "metadata": {},
   "source": [
    "We are primarily interested in only medium and large size airports based in US. So we will first filter the data for these criteria and then carry on with our analysis.  \n",
    "  \n",
    "Basic check on the data shows that there are no null values in the TYPE column and hence we do not need to do any pre processing/imputation on the TYPE column before filtering.  \n",
    "  \n",
    "**Data Limitation:** We can observe that there are **247 NULL** values in **ISO_COUNTRY** column. This is a potential issue since we want to **limit our analysis for domestic US airports**. We can observe that there is **NO NULL** value in the **COORDINATES** column. We can use this data to obtain the missing datapoints for ISO_COUNTRY columns.  \n",
    "  \n",
    "We will first filter the data by TYPE for 'Medium' and 'Large' size airports to see if the issue still exists"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e091b490",
   "metadata": {},
   "source": [
    "#### Filtering for Medium and Large airports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a211d081",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "airports['TYPE'].value_counts()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "61bd2c8e",
   "metadata": {},
   "source": [
    "There are **4532 Medium** sized airports which accounts for **8.1%** of the entire airport TYPE and **614 Large** sized airports which accounts for **1.1%** of the entire airport TYPE."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9335bea3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Filtering only the Large and Medium Sized airports\n",
    "filter_condition = ['large_airport','medium_airport']\n",
    "airport_subset = airports[airports['TYPE'].isin(filter_condition)]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e449ad9a",
   "metadata": {},
   "source": [
    "#### Plotting the Airport Types"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cf39c1ee",
   "metadata": {},
   "outputs": [],
   "source": [
    "sns.countplot(x = airport_subset[airport_subset['ISO_COUNTRY'].isin(['US','UM'])]['ISO_COUNTRY'],hue = airport_subset['TYPE'], data = airport_subset)\n",
    "plt.title(\"Number of Large & Medium Airport Types\")\n",
    "plt.xlabel(\"US/UM Country \")\n",
    "plt.ylabel(\"Number of aiport types\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c2829e63",
   "metadata": {},
   "source": [
    "#### Examining the NULL value counts again for the subset of the data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bdbe13c3",
   "metadata": {},
   "outputs": [],
   "source": [
    "airport_subset.isna().sum()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "35a8262d",
   "metadata": {},
   "source": [
    "After filtering for airport TYPE the null values present in the above columns dropped."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9f8afaac",
   "metadata": {},
   "source": [
    "**Percentage counts of Null Values**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e2a8040b",
   "metadata": {},
   "outputs": [],
   "source": [
    "round(airport_subset.isna().sum()*100/airport_subset.isna().sum().sum(),2)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c0206e23",
   "metadata": {},
   "source": [
    "We can see that the NULL value count dropped from **247 to 12**, which corresponds to **0.42%** of the data so we will ignore these rows and further filter by **ISO_COUNTRY = US**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "593639b0",
   "metadata": {},
   "source": [
    "#### Filtering for US based airports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "6cc4cd41",
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'airport_subset' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-9-39721770e0e4>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mairport_subset\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'ISO_COUNTRY'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mvalue_counts\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m: name 'airport_subset' is not defined"
     ]
    }
   ],
   "source": [
    "airport_subset['ISO_COUNTRY'].value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5898f773",
   "metadata": {},
   "outputs": [],
   "source": [
    "airport_subset[airport_subset['ISO_COUNTRY'].isin(['US','UM'])]['ISO_COUNTRY'].value_counts()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b82335c2",
   "metadata": {},
   "source": [
    "The ISO_COUNTRY code UM corresponds to US Minor Outlying Islands. There are only **2** such airports, hence we are not considering them for further analysis. Further, we make the ***assumption*** that all airports having the ISO_COUNTRY code US are **domestic airports**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3dbcda3b",
   "metadata": {},
   "outputs": [],
   "source": [
    "US_airport_subset = airport_subset[airport_subset['ISO_COUNTRY'] == 'US']"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c89c66a4",
   "metadata": {},
   "source": [
    "#### Size of the subset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "27fde4d2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Total number of rows that are of interest after filtering for the respective conditions are 858\n",
    "US_airport_subset.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8dcdd8ae",
   "metadata": {},
   "source": [
    "#### Checking for NULL IATA codes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "12b4e046",
   "metadata": {},
   "outputs": [],
   "source": [
    "US_airport_subset['IATA_CODE'].isna().sum()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c52a76bd",
   "metadata": {},
   "source": [
    "#### **We require IATA codes for joining the data with the ticket and the flight dataset. Hence we will drop the 37 rows with NULL IATA codes**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1b8cb4d1",
   "metadata": {},
   "outputs": [],
   "source": [
    "US_airport_subset = US_airport_subset[~US_airport_subset['IATA_CODE'].isna()]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6effb529",
   "metadata": {},
   "outputs": [],
   "source": [
    "US_airport_subset.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e684ce43",
   "metadata": {},
   "source": [
    "**We will use this subset of 821 US airports for further analysis.**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "12ebfb23",
   "metadata": {},
   "outputs": [],
   "source": [
    "# The present count of the numer of large and medium sized airports \n",
    "US_airport_subset['TYPE'].value_counts()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b9dfe288",
   "metadata": {},
   "source": [
    "## Exploring the Tickets data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fe0ad04f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Reading the Tickets.csv dataset\n",
    "tickets = pd.read_csv('Tickets.csv')\n",
    "# Shape of the dataset indicates there are 1167285 rows and 12 columns \n",
    "tickets.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f2fd24ca",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Veiwing the first 5 rows of the dataset in a transposed fashion to glance at all the columns \n",
    "tickets.head().T"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b03ad92a",
   "metadata": {},
   "source": [
    "#### Checking the dtypes and Missing Value counts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5b4f450d",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "tickets.dtypes"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c9363a40",
   "metadata": {},
   "source": [
    "All the columns are rightly aligned with the respective data type except for the ITIN FARE column which has to be casted to type float."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "14d769ca",
   "metadata": {},
   "outputs": [],
   "source": [
    "tickets.isna().sum()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "08df262b",
   "metadata": {},
   "source": [
    "The passenger column is not of interest for further calculations and hence not treated for null values. The ITIN FARE column has 960 Null values which has to be treated."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3f93d07c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plotting the missing value distribution\n",
    "msno.bar(tickets)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fb42506b",
   "metadata": {},
   "source": [
    "We observe that **ITIN_FARE** is of type **object**. We will have to clean this column. Also, **PASSENGERS** and **ITIN_FARE**, both these columns have **missing data**. We will have to impute the missing data appropriately only in the column of interest."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "878ec4cc",
   "metadata": {},
   "source": [
    "#### Filtering the data for round trips"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e1b88956",
   "metadata": {},
   "outputs": [],
   "source": [
    "tickets['ROUNDTRIP'].value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f0e2a7d8",
   "metadata": {},
   "outputs": [],
   "source": [
    "tickets = tickets[tickets['ROUNDTRIP'] == 1.0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d9d0ded0",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(f\"The number of itinerary after filtering for Round Trip Tickets: {tickets.shape[0]}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a067eb16",
   "metadata": {},
   "source": [
    "#### Filtering the tickets data for US Domestic Flights"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ef182feb",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Merging the tickets with the US_airport_subset data to include all IATA CODES for both ORIGIN and DESTINATION in the tickets dataset\n",
    "tickets = pd.merge(tickets,\n",
    "                   US_airport_subset['IATA_CODE'],\n",
    "                   left_on = 'ORIGIN',\n",
    "                   right_on = 'IATA_CODE',\n",
    "                   how = 'inner')\n",
    "\n",
    "tickets.drop(['IATA_CODE'], axis = 1, inplace = True)\n",
    "\n",
    "tickets = pd.merge(tickets,\n",
    "                   US_airport_subset['IATA_CODE'],\n",
    "                   left_on = 'DESTINATION',\n",
    "                   right_on = 'IATA_CODE',\n",
    "                   how = 'inner')\n",
    "tickets.drop(['IATA_CODE'], axis = 1, inplace = True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "76529ace",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(f\"The number of itinerary after cleaning the data for US Domestic round trips: {tickets.shape[0]}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "632a8c74",
   "metadata": {},
   "source": [
    "#### Checking for missing values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a9dc9ad9",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# As seen the filtering causes the number of missing values to drop\n",
    "tickets.isna().sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "26f88e61",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function to check if the value present in the ITIN_FARE column is clean or not. We define clean\n",
    "# value as being numeric containing only 1 decimal point\n",
    "\n",
    "def is_clean(value):\n",
    "    \n",
    "    clean = re.sub(r'[^0-9.]+','', str(value))\n",
    "    \n",
    "    if clean == value:\n",
    "        return 1\n",
    "    else:\n",
    "        return 0"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b4b4b9f3",
   "metadata": {},
   "source": [
    "We define a new column **'IS_CLEAN_FARE'** to see if the fares are clean or not. We will then inspect the unclean values to identify how to clean the data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a9dda552",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Apply the clean function to all values in ITIN_FARE\n",
    "tickets['IS_CLEAN_FARE'] = tickets['ITIN_FARE'].apply(lambda x: is_clean(x))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "eb5ed540",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Verify if the rows have not been cleaned by the clean function\n",
    "tickets[tickets['IS_CLEAN_FARE'] == 0].sample(20)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1a85b851",
   "metadata": {},
   "source": [
    "#### Data Quality Issue\n",
    "\n",
    "We can see that most of the unclean data is occuring because of the presence of '$' sign either at the beginning or the end of the string. Checks must be put in place while recording this value so that only numeric entries are allowed"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0f727299",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function to clean the value present in the ITIN_FARE column. A clean value should have\n",
    "# only numeric values with only 1 decimal point\n",
    "\n",
    "def clean_value(value):\n",
    "    \n",
    "    clean = re.sub(r'[^0-9.]+','', str(value))\n",
    "    \n",
    "    try:\n",
    "        clean = clean.split('.')[0]\n",
    "    except:\n",
    "        clean = np.nan\n",
    "    \n",
    "    return clean"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ccb87b7f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cleaning and converting the data type of ITIN_FARE\n",
    "tickets['ITIN_FARE'] = tickets['ITIN_FARE'].apply(lambda x: clean_value(x))\n",
    "tickets['ITIN_FARE'] = tickets['ITIN_FARE'].replace('',np.nan).astype('float64')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f92fc897",
   "metadata": {},
   "outputs": [],
   "source": [
    "tickets['ITIN_FARE'].describe()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0111d9a9",
   "metadata": {},
   "source": [
    "#### Defining a new column 'TRIP' that corresponds to the round trip between ORIGIN and the DESTINATION column\n",
    "\n",
    "For eg. if the ORIGIN is SFO and the destination is LAX. The trip column will have the value 'SFO-LAX'\n",
    "\n",
    "This column will be used to analyse the data at a round trip level. \n",
    "\n",
    "**Missing value treatment**  \n",
    "Besides, we will use the mean fare values for each round trip to impute the **missing data** for that particular round trip. This will ensure that the imputed values corresponds to the average behaviour for that particular round trip. We use the average because the mean and the median values are pretty close to each other."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3ae3d967",
   "metadata": {},
   "outputs": [],
   "source": [
    "# A roundtrip column defined by the origin and destination of the tickets dataset\n",
    "tickets['TRIP'] = tickets['ORIGIN'] + '-' + tickets['DESTINATION']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ebd07cfa",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Understand how many unique roundtrips are present\n",
    "tickets['TRIP'].nunique()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dfef1465",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Calculating the mean of the ITIN_FARE for each roundtrip and renaming the column as AVG_FARE\n",
    "avg_ticket_price = tickets.groupby('TRIP')['ITIN_FARE'].mean().reset_index().rename(columns={'ITIN_FARE':'AVG_FARE'})\n",
    "avg_ticket_price"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bc2e122f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Checking for Null values in AVG_TICKET PRICE Dataset\n",
    "avg_ticket_price.isna().sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "482a6519",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Displaying the trips which have a Null Value in AVG_FARE\n",
    "avg_ticket_price[avg_ticket_price['AVG_FARE'].isna()]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "059ced0c",
   "metadata": {},
   "source": [
    "We observe that **8** round trips have **no fare** information for them. We fill these missing values with the **median** ticket price for a round trip."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3f73bf1a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Displaying the trips and the respective columns where ITIN_FARE(that corresponds to AVG_FARE) is Null\n",
    "tickets[tickets['TRIP'].isin(avg_ticket_price[avg_ticket_price['AVG_FARE'].isna()]['TRIP'])]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "beae1690",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Filling the missing values with the median of AVG_FARE\n",
    "avg_ticket_price['AVG_FARE'] = avg_ticket_price['AVG_FARE'].fillna(avg_ticket_price['AVG_FARE'].median())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d56ae967",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Merging the tickets dataset on the Trip column with the Avg_ticket_price dataset\n",
    "tickets = pd.merge(tickets,\n",
    "                   avg_ticket_price,\n",
    "                   on = 'TRIP',\n",
    "                   how = 'left')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "45fb4d3d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# We define a new column 'ITIN_FARE_CLEAN' where we fill the missing values \n",
    "# with the corresponding average round trip fare \n",
    "tickets['ITIN_FARE_CLEAN'] = np.where(tickets['ITIN_FARE'].isna(), tickets['AVG_FARE'], tickets['ITIN_FARE'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4fa8ed3a",
   "metadata": {},
   "outputs": [],
   "source": [
    "tickets['ITIN_FARE_CLEAN'].describe()\n",
    "# As seen that the mean and median in the ITIN FARE CLEAN column is very close to each other impying that this is a normal distribution rather than a skewed one."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "53118f86",
   "metadata": {},
   "source": [
    "#### Outlier Detection "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4426f93c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plotting a histogram to detect outliers\n",
    "sns.histplot(tickets['ITIN_FARE_CLEAN'])\n",
    "plt.title(\"Outliers in the ITIN FARE Column\")\n",
    "plt.xlabel(\"ITIN FARE\")\n",
    "plt.ylabel(\"Number of tickets\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4e8c3b6f",
   "metadata": {},
   "source": [
    "#### Removing the outliers in the tickets data\n",
    "\n",
    "Based on the graph, we can see that the there are outliers present in the fares. We will remove the bottom and top 0.5% records and work with 99% of the data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "aa0d1b89",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Applying the condtion for outlier elimination\n",
    "tickets = tickets[(tickets['ITIN_FARE_CLEAN'] <= tickets['ITIN_FARE_CLEAN'].quantile(0.995)) & \n",
    "                  (tickets['ITIN_FARE_CLEAN'] > tickets['ITIN_FARE_CLEAN'].quantile(0.005))]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "78515ff6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plotting the ITIN_FARE_CLEAN with no outliers\n",
    "sns.histplot(tickets['ITIN_FARE_CLEAN'])\n",
    "plt.title(\"Removal of outilers in ITIN FARE \")\n",
    "plt.xlabel(\"ITIN FARE\")\n",
    "plt.ylabel(\"Number of tickets\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "54d7de0b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Understand relationship between different carriers and the respective fare\n",
    "sns.boxplot(x = tickets['REPORTING_CARRIER'],\n",
    "            y = tickets['ITIN_FARE_CLEAN'])\n",
    "plt.title(\"Relationship between Airlines and Fare Charged per Passenger\")\n",
    "plt.xlabel(\"Airlines\")\n",
    "plt.ylabel(\"Fare Charged per Passenger by the Airline in USD\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e42423e0",
   "metadata": {},
   "source": [
    "#### We can see that the fares on average cost ~500 USD. With most of the airlines charging roughly the same amount for a roundtrip except the airlines that have either monopoly operating in a particular region or are cost effecient like Frontier Airlines(F9), Spirit Airlines(NK), Compass Airlines(CP), Horizon Air(QX), Southwest Airlines(WN), Sky West Airlines(SY), Hawaiian Airlines(HA) and Allegiant Air(G4)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8c3e3fff",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Displaying the first 5 rows of tickets with all the columns in the merged ticket dataset\n",
    "tickets.head().T"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "327e65ac",
   "metadata": {},
   "source": [
    "## Exploring the Flights Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "24c3f219",
   "metadata": {},
   "outputs": [],
   "source": [
    "# From the data dictionary we see that 'FL_DATE' is a date column and hence we parse it into a datetype object \n",
    "# while loading the dataset\n",
    "flights = pd.read_csv('Flights.csv', parse_dates=['FL_DATE'])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f7332c8e",
   "metadata": {},
   "source": [
    "#### Size of the data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d61c46e9",
   "metadata": {},
   "outputs": [],
   "source": [
    "flights.shape\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6e222d8d",
   "metadata": {},
   "source": [
    "We have data corresponding to ~1.9 million flights."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ea7ed1f4",
   "metadata": {},
   "source": [
    "#### Sample of the data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e39c8383",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Looking at a sample data in a transposed fashion\n",
    "flights.head().T"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "51ffd9e3",
   "metadata": {},
   "source": [
    "The columns ORIGIN and DESTINATION corresponds to the IATA codes assigned to the aiports. We will use these columns to filter the data for Domestic US based flights"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7fbd98e6",
   "metadata": {},
   "source": [
    "#### Checking the data types and missing value counts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "40656515",
   "metadata": {},
   "outputs": [],
   "source": [
    "flights.dtypes"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "921c9884",
   "metadata": {},
   "source": [
    "#### Data Quality Issue:\n",
    "\n",
    "**AIR_TIME** and **DISTANCE** needs to be in float/int format. Besides that, the rest of the columns looks appropriate"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fe348f0b",
   "metadata": {},
   "outputs": [],
   "source": [
    "flights.isna().sum()\n",
    "msno.bar(flights)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "507da616",
   "metadata": {},
   "source": [
    "There is no null data in ORIGIN, DESTINATION (IATA CODES) and CANCELLED column. Hence, we do not need to do data imputation before filtering the data for further analysis"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1e394905",
   "metadata": {},
   "source": [
    "#### Checking the distribution of cancelled flights"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "37a3d09c",
   "metadata": {},
   "outputs": [],
   "source": [
    "flights['CANCELLED'].value_counts()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bf44fcaa",
   "metadata": {},
   "source": [
    "Out of ~1.9 million flights, ~51.6K flights have been cancelled implies a cancellation rate of approx 3% which adheares to the normal airline standards. We ignore the cancelled flights and use the remaining set to filter for US based domestic flights. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8241c2dc",
   "metadata": {},
   "source": [
    "#### Filtering the CANCELLED flights"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "a4ed1dfa",
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'flights' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-10-8233a0afd993>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mflights\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mflights\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mflights\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'CANCELLED'\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m!=\u001b[0m \u001b[0;36m1.0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      2\u001b[0m \u001b[0mflights\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshape\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0;31m# Number of the flights that are not cancelled is 1864272\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mNameError\u001b[0m: name 'flights' is not defined"
     ]
    }
   ],
   "source": [
    "flights = flights[flights['CANCELLED'] != 1.0]\n",
    "flights.shape\n",
    "# Number of the flights that are not cancelled is 1864272"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9953b670",
   "metadata": {},
   "source": [
    "#### FILTERING the ORIGIN and DESTINATION and MERGING the AIRPORT data\n",
    "\n",
    "We are interested in domestic US Flights Market. Hence, we will filter the **ORIGIN** and the **DESTINATION** column by US IATA codes we identified in the airport data analysis."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1552150e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Merging the flights dataset with the US_airport_subset data to include all IATA CODES for both ORIGIN and DESTINATION in the flights dataset\n",
    "flights = pd.merge(flights,\n",
    "                   US_airport_subset['IATA_CODE'],\n",
    "                   left_on = 'ORIGIN',\n",
    "                   right_on = 'IATA_CODE',\n",
    "                   how = 'inner')\n",
    "\n",
    "flights.drop(['IATA_CODE'], axis = 1, inplace=True)\n",
    "\n",
    "flights = pd.merge(flights,\n",
    "                   US_airport_subset['IATA_CODE'],\n",
    "                   left_on = 'DESTINATION',\n",
    "                   right_on = 'IATA_CODE',\n",
    "                   how = 'inner')\n",
    "\n",
    "flights.drop(['IATA_CODE'], axis = 1, inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "89667668",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Shape of the flighta dataset\n",
    "flights.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bca73f33",
   "metadata": {},
   "source": [
    "### Pre-processing and cleaning the flights data\n",
    "\n",
    "We define the same column 'TRIP' as we defined above to identify a journey"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f4ee3915",
   "metadata": {},
   "outputs": [],
   "source": [
    "flights['TRIP'] = flights['ORIGIN'] + '-' + flights['DESTINATION']"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "07e5668c",
   "metadata": {},
   "source": [
    "#### Dtype Correction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1adb0393",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Applying the clean function to the DISTANCE and AIR_TIME columsn in the flights data set and cast it to float\n",
    "flights['DISTANCE'] = flights['DISTANCE'].apply(lambda x: clean_value(x))\n",
    "flights['AIR_TIME'] = flights['AIR_TIME'].apply(lambda x: clean_value(x))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "124d33e7",
   "metadata": {},
   "outputs": [],
   "source": [
    "flights['DISTANCE'] = flights['DISTANCE'].replace('',np.nan).astype('float64')\n",
    "flights['AIR_TIME'] = flights['AIR_TIME'].replace('',np.nan).astype('float64')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6415705d",
   "metadata": {},
   "source": [
    "**Checking for missing values in the filtered data. We will impute the missing data using average/median statistics for the corresponding round trip**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e880b370",
   "metadata": {},
   "outputs": [],
   "source": [
    "flights.isna().sum()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fd53165f",
   "metadata": {},
   "source": [
    "#### Outlier Detection"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cf9a37e7",
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "# Detecting outliers in the Air time column\n",
    "sns.histplot(flights['AIR_TIME'])\n",
    "plt.title(\"Outliers in the AIR TIME column\")\n",
    "plt.xlabel(\"Air Time in mins\")\n",
    "plt.ylabel(\"Number of Flights\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e4759015",
   "metadata": {},
   "outputs": [],
   "source": [
    "# From the plot it can be seen that 890 values > 500 mins of air time are not useful for analysis\n",
    "flights[flights['AIR_TIME'] >= 500].shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "99b1bc11",
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "# Detecting outliers in the arrival delay column\n",
    "sns.histplot(flights['ARR_DELAY'])\n",
    "plt.title(\"Outliers in the ARR DELAY column\")\n",
    "plt.xlabel(\"Arrival Delay in mins\")\n",
    "plt.ylabel(\"Number of Flights\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b17d0d97",
   "metadata": {},
   "outputs": [],
   "source": [
    "# From the plot it can be seen that 2681 values > 500 mins of arrival delay are not useful for analysis\n",
    "flights[flights['ARR_DELAY'] >= 500].shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "51ece402",
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "# Detecting outliers in the departure delay column\n",
    "sns.histplot(flights['DEP_DELAY'])\n",
    "plt.title(\"Outliers in the DEP DELAY column\")\n",
    "plt.xlabel(\"Departure Delay in mins\")\n",
    "plt.ylabel(\"Number of Flights\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "09e01439",
   "metadata": {},
   "outputs": [],
   "source": [
    "# From the plot it can be seen that 2723 values > 500 mins of departure delay are not useful for analysis\n",
    "flights[flights['DEP_DELAY'] >= 500].shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "99e1552b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Detecting outliers in the distance column\n",
    "sns.histplot(flights['DISTANCE'])\n",
    "plt.title(\"Outliers in the Distance column\")\n",
    "plt.xlabel(\"Distance Travelled by flight in miles\")\n",
    "plt.ylabel(\"Number of Flights\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7f0db7b0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# From the plot it can be seen that 30 values > 5000 miles of distance are not useful for analysis\n",
    "flights[flights['DISTANCE'] >= 5000].shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "409d7b26",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Detecting outliers in the distance column\n",
    "sns.histplot(flights['OCCUPANCY_RATE'])\n",
    "plt.title(\"Outliers in the Occupancy Rate column\")\n",
    "plt.xlabel(\"Occuapncy Rate in each flight\")\n",
    "plt.ylabel(\"Number of Flights\")\n",
    "# From the plot it can be seen that there are no outliers present and the data is uniformly distributed accross the entire dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3928b042",
   "metadata": {},
   "source": [
    "#### Removing outliers\n",
    "\n",
    "We remove outliers based on the graphs above. We define the cut off value for arrival and departure delays at 500 minutes, air time at 500 minutes and distance at 5000 miles"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "28c63bd3",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(f\"Number of records before removing outliers: {flights.shape[0]}\")\n",
    "flights = flights[(flights['DEP_DELAY'] <= 500) &\n",
    "                  (flights['ARR_DELAY'] <= 500) &\n",
    "                  (flights['DISTANCE'] <= 5000) & \n",
    "                  (flights['AIR_TIME'] <= 500)]\n",
    "print(f\"Number of records after removing outliers: {flights.shape[0]}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "55a32bb4",
   "metadata": {},
   "source": [
    "#### Missing value treatment\n",
    "\n",
    "Besides, OCCUPANCY_RATE, all other columns are right skewed. Hence, we take median of these dsitrbutions by each trip and then impute the null values "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b17e505d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Imputing the median for all of these columns for each trip\n",
    "avg_flights_stat = flights.groupby('TRIP')['ARR_DELAY','AIR_TIME','DISTANCE','OCCUPANCY_RATE'].median().reset_index()\n",
    "avg_flights_stat.head().T"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6f154e68",
   "metadata": {},
   "source": [
    "#### Checking for missing values in the aggregare data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4b5811ee",
   "metadata": {},
   "outputs": [],
   "source": [
    "avg_flights_stat.isna().sum()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "401e1b78",
   "metadata": {},
   "source": [
    "**Imputing the missing aggregate data using the average behavior of the all trips**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4e0c0988",
   "metadata": {},
   "outputs": [],
   "source": [
    "avg_flights_stat['ARR_DELAY'] = avg_flights_stat['ARR_DELAY'].fillna(avg_flights_stat['ARR_DELAY'].mean())\n",
    "avg_flights_stat['AIR_TIME'] = avg_flights_stat['AIR_TIME'].fillna(avg_flights_stat['AIR_TIME'].mean())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5d3b3a5c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Renaming the columns to the appropriate names\n",
    "avg_flights_stat.rename(columns={'ARR_DELAY':'MEDIAN_ARR_DELAY',\n",
    "                                 'AIR_TIME':'MEDIAN_AIR_TIME',\n",
    "                                 'DISTANCE':'MEDIAN_DISTANCE',\n",
    "                                 'OCCUPANCY_RATE':'MEDIAN_OCCUPANCY_RATE'},\n",
    "                        inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "55372c6c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Merging the data \n",
    "flights = pd.merge(flights,\n",
    "                   avg_flights_stat,\n",
    "                   on = 'TRIP',\n",
    "                   how = 'inner')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e5e2e700",
   "metadata": {},
   "source": [
    "**Imputing the missing values with the median value for the corresponding trip**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f3b4245a",
   "metadata": {},
   "outputs": [],
   "source": [
    "flights['ARR_DELAY'] = np.where(flights['ARR_DELAY'].isna(),\n",
    "                                flights['MEDIAN_ARR_DELAY'],\n",
    "                                flights['ARR_DELAY'])\n",
    "\n",
    "flights['AIR_TIME'] = np.where(flights['AIR_TIME'].isna(),\n",
    "                               flights['MEDIAN_AIR_TIME'],\n",
    "                               flights['AIR_TIME'])\n",
    "\n",
    "flights['DISTANCE'] = np.where(flights['DISTANCE'].isna(),\n",
    "                               flights['MEDIAN_DISTANCE'],\n",
    "                               flights['DISTANCE'])\n",
    "\n",
    "flights['OCCUPANCY_RATE'] = np.where(flights['OCCUPANCY_RATE'].isna(),\n",
    "                                     flights['MEDIAN_OCCUPANCY_RATE'],\n",
    "                                     flights['OCCUPANCY_RATE'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "741eb23b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Check for missing values as it can be seen they have been dealt with\n",
    "flights.isna().sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6d3b02ec",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Relationship between the airlines and the distance flown by them\n",
    "sns.boxplot(x = flights['OP_CARRIER'],\n",
    "            y = flights['DISTANCE'])\n",
    "plt.title(\"Relationship between Airlines and respective Distance \")\n",
    "plt.xlabel(\"Airlines\")\n",
    "plt.ylabel(\"Distance flown by the airlines in miles\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5ae06eec",
   "metadata": {},
   "source": [
    "Most airlines operate under 3000 miles flight distance, with an exception of United Airlines, Delta Airlines, American Airlines and Hawaian Airlines"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1cd841bb",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Relationship between the airlines and the departure delay for that respective airline\n",
    "sns.boxplot(x = flights['OP_CARRIER'],\n",
    "            y = flights['DEP_DELAY'])\n",
    "plt.title(\"Relationship between Airlines and respective Departure Delay \")\n",
    "plt.xlabel(\"Airlines\")\n",
    "plt.ylabel(\"Delay in Departure by each flight in mins\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "22582040",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Relationship between the airlines and the arrival delay for that respective airline\n",
    "sns.boxplot(x = flights['OP_CARRIER'],\n",
    "            y = flights['ARR_DELAY'])\n",
    "plt.title(\"Relationship between Airlines and respective Arrival Delay \")\n",
    "plt.xlabel(\"Airlines\")\n",
    "plt.ylabel(\"Delay in Arrival by each flight in mins\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cfd0016c",
   "metadata": {},
   "source": [
    "We can see from the above box plots that not many carriers have either an arrival or departure delay except for NLA Penair(KS) and UCA Champlain Air(C5) have delay in arrival and departure; along with Jetblue Airways(B6),Frontier Airlines(F9) and Trans State Airlines(AX) have a delay in arrival as well."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "128bb77a",
   "metadata": {},
   "source": [
    "After examining all three datasets it can be noted that:\n",
    "\n",
    "1. **Accuracy**: Outlier Detection deals with removing the outliers that are insensitive to business model metric.\n",
    "\n",
    "2. **Completeness**: Data quality check and missing value treatment takes care of any incomplete data issues.\n",
    "\n",
    "Flights Dataset has outliers in four columns namely; Air Time have outliers > **500 mins**, Arr Delay have outliers > **500 mins**, Distance have outliers > **5000 miles**, Dep Delay have outliers > **500 mins** as seen from the histogram plot and is respectively treated by outlier elimination. The missing values are identified and treated using median imputation for the columns Air Delay with **4271** Null values , Air Time with **6751** Null values, Distance with **2680** Null Values and Occupancy Rate with **310** Null values. Data quality issues are present in the Air Time and Distance column which is cleaned by the explicitly defined clean function.\n",
    "\n",
    "Tickets Dataset has outliers in the ITIN FARE Clean coloumn thus the **bottom and top 0.5% records** are removed and work with **99% of the data** as seen from the histogram plot and is treated by outlier elimination. The missing values are present in the ITIN_FARE with **545** Null Values and Passenger column with **1162** Null Values. Since Passenger column is irrelevant to calculate the evaluation metircs thus no further missing value treatment is performed on it. Missing values are treated using Median Imputation for the ITIN FARE column. Data quality issues are handled in the Fare column by the explicity defined clean function. \n",
    "\n",
    "Airports Dataset has missing values in the TYPE column which can be dropped as it accounts for only **0.42%** of the total number of rows respectively after applying the filtering condition that the **ISO Country is limited to US only**. \n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b0e4653a",
   "metadata": {},
   "source": [
    "# Defining the Metrics\n",
    "\n",
    "## Calculating the cost\n",
    "\n",
    "### Adding base costs to the airport data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "689ad1c8",
   "metadata": {},
   "outputs": [],
   "source": [
    "US_airport_subset.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1ac758c1",
   "metadata": {},
   "source": [
    "**We define columns 'COST','LAT' & 'LONG' which corresponds to the usage charges for the airport and the latitute and the longitude**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8c9ec7c1",
   "metadata": {},
   "outputs": [],
   "source": [
    "US_airport_subset['COST'] = np.where(US_airport_subset['TYPE'] == 'medium_airport', 5000, 10000)\n",
    "US_airport_subset['LAT'] = US_airport_subset['COORDINATES'].apply(lambda x: float(x.split(',')[0]))\n",
    "US_airport_subset['LONG'] = US_airport_subset['COORDINATES'].apply(lambda x: float(x.split(',')[1]))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2bd70947",
   "metadata": {},
   "source": [
    "### Adding late cost and misc cost to the flights data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "64424951",
   "metadata": {},
   "outputs": [],
   "source": [
    "flights.head().T"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "da5a1215",
   "metadata": {},
   "source": [
    "**Defining COSTS associated to operating a flight**  \n",
    "  \n",
    "**MISC COST:** 8 USD per mile (Fuel, Oil, Maintenance, Crew) + 1.18 USD per mile (Depreciation, Insurance, Other)  \n",
    "**DEP_LATE_FEES:** Late Fees charged to the airline for delay in departure. First 15 minutes delay are free. Post that 75 USD charges on each minute delay  \n",
    "**ARR_LATE_FEES:** Late Fees charged to the airline for delay in arrival. First 15 minutes delay are free. Post that 75 USD charges on each minute delay  \n",
    "**LATE_FEES:** DEP_LATE_FEES + ARR_LATE_FEES  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3e256f07",
   "metadata": {},
   "outputs": [],
   "source": [
    "# $8 per mile (Fuel, Oil, Maintenance, Crew) + $1.18 per mile (Depreciation, Insurance, Other)\n",
    "flights['MISC_COST'] = flights['DISTANCE'] * 9.18"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "51020dad",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Flights that have an arrival of departure delay more than 15 mins are charged $75 per mintute\n",
    "flights['DEP_LATE_FEES'] = np.where(flights['DEP_DELAY'] > 15, \n",
    "                                    75*np.ceil(flights['DEP_DELAY'] - 15),\n",
    "                                    0)\n",
    "\n",
    "flights['ARR_LATE_FEES'] = np.where(flights['ARR_DELAY'] > 15, \n",
    "                                    75*np.ceil(flights['ARR_DELAY'] - 15),\n",
    "                                    0)\n",
    "\n",
    "flights['LATE_FEES'] = flights['DEP_LATE_FEES'] + flights['ARR_LATE_FEES']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5b0cdc5a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Merging the flights dataset with the US_airport_subset with the Cost column in place\n",
    "flights = pd.merge(flights,\n",
    "                   US_airport_subset[['IATA_CODE','COST']],\n",
    "                   left_on = 'DESTINATION',\n",
    "                   right_on = 'IATA_CODE',\n",
    "                   how = 'inner')\n",
    "flights.drop(['IATA_CODE'], axis = 1, inplace = True)\n",
    "flights.rename(columns = {'COST':'DESTINATION_COST'}, inplace = True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a4bcb9d8",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Display the various stats for the columns to understand the dollar amount spent for each.\n",
    "flights[['DEP_LATE_FEES','ARR_LATE_FEES','LATE_FEES','MISC_COST']].describe().apply(lambda s: s.apply('{0:.5f}'.format))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f1329ee7",
   "metadata": {},
   "source": [
    "We define a new column **IS_LATE** to calculate the statistics for flights that are late"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3cba6191",
   "metadata": {},
   "outputs": [],
   "source": [
    "flights['IS_LATE'] = flights.apply(lambda row: 1 if (row['DEP_DELAY'] > 0 or row['ARR_DELAY'] > 0) else 0, axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b00b5de7",
   "metadata": {},
   "outputs": [],
   "source": [
    "flights[flights['IS_LATE'] == 1][['DEP_DELAY','DEP_LATE_FEES']].describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4b045c83",
   "metadata": {},
   "outputs": [],
   "source": [
    "flights[flights['IS_LATE'] == 1][['ARR_DELAY','ARR_LATE_FEES']].describe()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4e1be444",
   "metadata": {},
   "source": [
    "#### Aggregating the flight data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "55638aa2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Aggrgating the flights dataset by each trip with the respective columns with mean \n",
    "flights_agg = flights.groupby(['ORIGIN','DESTINATION','TRIP']).agg({'FL_DATE':'count',\n",
    "                                                                    'OP_CARRIER':'nunique',\n",
    "                                                                    'MISC_COST':'mean',\n",
    "                                                                    'ARR_DELAY':'median',\n",
    "                                                                    'DEP_DELAY':'median',\n",
    "                                                                    'ARR_LATE_FEES':'median',\n",
    "                                                                    'DEP_LATE_FEES':'median',\n",
    "                                                                    'LATE_FEES':'median',\n",
    "                                                                    'DESTINATION_COST':'mean',\n",
    "                                                                    'OCCUPANCY_RATE':'mean',\n",
    "                                                                    'DISTANCE':'mean',\n",
    "                                                                    'AIR_TIME':'mean'}).reset_index()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ef4fea39",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Display the flight dataset\n",
    "flights_agg.head().T"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8381a1ca",
   "metadata": {},
   "outputs": [],
   "source": [
    "# A function which takes into account that each trip in a round trip is different and reverses the first leg of the roundtrip to get the second leg of the roundtrip\n",
    "def make_key(value):\n",
    "    \n",
    "    origin = value.split('-')[0]\n",
    "    destination = value.split('-')[1]\n",
    "    \n",
    "    if origin < destination:\n",
    "        ret_val = value\n",
    "    else:\n",
    "        ret_val = destination + '-' + origin\n",
    "        \n",
    "    return ret_val"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4e1785c5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Applying the fucntion on a new created column called KEY\n",
    "flights_agg['KEY'] = flights_agg['TRIP'].apply(lambda x: make_key(x))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2db6d283",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Merging the flights_agg dataset on itself to account for both the legs in a roundtrip\n",
    "\n",
    "round_trip_flights = pd.merge(flights_agg,\n",
    "                              flights_agg,\n",
    "                              on = 'KEY',\n",
    "                              how = 'inner')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a5d81060",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Removing the unnecessary duplicate data points due to self join and keeping only the ones that differ\n",
    "round_trip_flights = round_trip_flights[(round_trip_flights['ORIGIN_x'] != round_trip_flights['ORIGIN_y']) &\n",
    "                                        (round_trip_flights['DESTINATION_x'] != round_trip_flights['DESTINATION_y'])]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "459fb9c1",
   "metadata": {},
   "outputs": [],
   "source": [
    "round_trip_flights.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b668b18e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Renaming all the coloumns for the final dataset as the both the legs of the roundtrip should have differnt data point nomenclature \n",
    "cols_rename={'ORIGIN_x':'ONWARD_ORIGIN',\n",
    "             'DESTINATION_x':'ONWARD_DESTINATION',\n",
    "             'TRIP_x':'ONWARD_TRIP',\n",
    "             'FL_DATE_x':'ONWARD_COUNT',\n",
    "             'OP_CARRIER_x':'ONWARD_NUM_AIRLINES',\n",
    "             'MISC_COST_x':'ONWARD_MISC_COST',\n",
    "             'ARR_DELAY_x':'ONWARD_ARR_DELAY',\n",
    "             'DEP_DELAY_x':'ONWARD_DEP_DELAY',\n",
    "             'ARR_LATE_FEES_x':'ONWARD_ARR_LATE_FEES',\n",
    "             'DEP_LATE_FEES_x':'ONWARD_DEP_LATE_FEES',\n",
    "             'LATE_FEES_x':'ONWARD_LATE_FEES',\n",
    "             'DESTINATION_COST_x':'ONWARD_DESTINATION_COST',\n",
    "             'OCCUPANCY_RATE_x':'ONWARD_OCCUPANCY_RATE',\n",
    "             'DISTANCE_x':'ONWARD_DISTANCE',\n",
    "             'AIR_TIME_x':'ONWARD_AIRTIME',\n",
    "             'KEY':'ROUND_TRIP',\n",
    "             'ORIGIN_y':'RETURN_ORIGIN',\n",
    "             'DESTINATION_y':'RETURN_DESTINATION',\n",
    "             'TRIP_y':'RETURN_TRIP',\n",
    "             'FL_DATE_y':'RETURN_COUNT',\n",
    "             'OP_CARRIER_y':'RETURN_NUM_AIRLINES',\n",
    "             'MISC_COST_y':'RETURN_MISC_COST',\n",
    "             'ARR_DELAY_y':'RETURN_ARR_DELAY',\n",
    "             'DEP_DELAY_y':'RETURN_DEP_DELAY',\n",
    "             'ARR_LATE_FEES_y':'RETURN_ARR_LATE_FEES',\n",
    "             'DEP_LATE_FEES_y':'RETURN_DEP_LATE_FEES',\n",
    "             'LATE_FEES_y':'RETURN_LATE_FEES',\n",
    "             'DESTINATION_COST_y':'RETURN_DESTINATION_COST',\n",
    "             'OCCUPANCY_RATE_y':'RETURN_OCCUPANCY_RATE',\n",
    "             'DISTANCE_y':'RETURN_DISTANCE',\n",
    "             'AIR_TIME_y':'RETURN_AIRTIME'}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7c284e46",
   "metadata": {},
   "outputs": [],
   "source": [
    "round_trip_flights.rename(columns = cols_rename, inplace = True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "04ca4520",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Considering the average of few data points when considering first and second leg of the roundtrip\n",
    "round_trip_flights['AVG_ARR_DELAY'] = (round_trip_flights['ONWARD_ARR_DELAY'] + \\\n",
    "                                       round_trip_flights['RETURN_ARR_DELAY'])/2\n",
    "round_trip_flights['AVG_DEP_DELAY'] = (round_trip_flights['ONWARD_DEP_DELAY'] + \\\n",
    "                                       round_trip_flights['RETURN_DEP_DELAY'])/2\n",
    "round_trip_flights['AVG_OCCUPANCY_RATE'] = (round_trip_flights['ONWARD_OCCUPANCY_RATE'] + \\\n",
    "                                       round_trip_flights['RETURN_OCCUPANCY_RATE'])/2\n",
    "round_trip_flights['DISTANCE'] = (round_trip_flights['ONWARD_DISTANCE'] + \\\n",
    "                                       round_trip_flights['RETURN_DISTANCE'])/2\n",
    "round_trip_flights['AIRTIME'] = (round_trip_flights['ONWARD_AIRTIME'] + \\\n",
    "                                       round_trip_flights['RETURN_AIRTIME'])/2\n",
    "round_trip_flights['NUM_AIRLINES'] = (round_trip_flights['ONWARD_NUM_AIRLINES'] + \\\n",
    "                                       round_trip_flights['RETURN_NUM_AIRLINES'])/2\n",
    "round_trip_flights['FLIGHT_COUNT'] = round_trip_flights.apply(lambda row: min(row['ONWARD_COUNT'], \n",
    "                                                                              row['RETURN_COUNT']), \n",
    "                                                              axis=1)\n",
    "round_trip_flights['LATE_FEES'] = round_trip_flights['ONWARD_LATE_FEES'] + round_trip_flights['RETURN_LATE_FEES']\n",
    "round_trip_flights['AIRPORT_COST'] = (round_trip_flights['ONWARD_DESTINATION_COST'] + \\\n",
    "                                      round_trip_flights['RETURN_DESTINATION_COST'])\n",
    "round_trip_flights['MISC_COST'] = round_trip_flights['ONWARD_MISC_COST'] + round_trip_flights['RETURN_MISC_COST']\n",
    "round_trip_flights['TRIP_COST'] = (round_trip_flights['LATE_FEES'] + round_trip_flights['AIRPORT_COST'] + \\\n",
    "                                    round_trip_flights['MISC_COST'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4d446c8e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Dropping any duplicate columns to main consistency in the final dataset\n",
    "keep_cols = ['ROUND_TRIP','FLIGHT_COUNT','NUM_AIRLINES','AVG_ARR_DELAY','AVG_DEP_DELAY','DISTANCE','AIRTIME',\n",
    "             'AVG_OCCUPANCY_RATE','AIRPORT_COST','MISC_COST','LATE_FEES','TRIP_COST']\n",
    "round_trips = round_trip_flights[keep_cols].drop_duplicates()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1079e776",
   "metadata": {},
   "outputs": [],
   "source": [
    "round_trips.head().T"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "865ce4b4",
   "metadata": {},
   "source": [
    "#### Outlier Detection"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "95e6eb56",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Detecting outliers present in the Miscellaneous Cost column \n",
    "sns.histplot(round_trips['MISC_COST'])\n",
    "plt.title(\"Outliers in the Miscellaneous Cost column\")\n",
    "plt.xlabel(\"Miscellaneous Cost per Passenger in USD\")\n",
    "plt.ylabel(\"Number of Trips\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b52a9741",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Detecting outliers present in the Trip Cost column \n",
    "sns.histplot(round_trips['TRIP_COST'])\n",
    "plt.title(\"Outliers in the Trip Cost column\")\n",
    "plt.xlabel(\"Trip Cost in USD\")\n",
    "plt.ylabel(\"Number of Trips\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0ec4d324",
   "metadata": {},
   "source": [
    "**On average it costs ~10,000 USD in Miscellenous expenses, ~15000 USD in Airport Charges and ~30000 USD for overall ROUND TRIP expense. This amount is directly proportional to the distance between the two airports.**\n",
    "\n",
    "As seen from the plots there are few data points that are present all through the cost axis for both miscellenous and trip. Hence it is not in the best interest to get rid of those outliers for further analysis and evaluation of metrics."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b5b45b28",
   "metadata": {},
   "source": [
    "## Revenue Metrics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b89783da",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Display the tickets dataset\n",
    "tickets.head().T"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d532802d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# The mean and median for the ITIN FARE CLEAN column are not that far apart implying that for each trip mean fare can be considered.\n",
    "tickets[['PASSENGERS','ITIN_FARE_CLEAN']].describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "43eeebc9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Applying the fucntion on a new created column called KEY\n",
    "tickets['ROUND_TRIP'] = tickets['TRIP'].apply(lambda x: make_key(x))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "50b971a9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Calculating the mean of fare for each trip and renaming the fare column to fare per passenger \n",
    "tickets_agg = tickets.groupby('ROUND_TRIP').agg({'ITIN_FARE_CLEAN':'mean'}).reset_index()\n",
    "tickets_agg.rename(columns={'ITIN_FARE_CLEAN':'FARE_PER_PASSENGER'},inplace = True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9e5dfb3d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Display of the sample data\n",
    "tickets_agg.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a317f0b8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# The tickets agg dataset consists of 23901 rows of interest\n",
    "tickets_agg.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d66a152f",
   "metadata": {},
   "source": [
    "### Data Munging"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "43c61138",
   "metadata": {},
   "outputs": [],
   "source": [
    "trip_data = pd.merge(round_trips,\n",
    "                     tickets_agg,\n",
    "                     on = 'ROUND_TRIP',\n",
    "                     how = 'inner')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e955bc07",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Trip data on merging with tickets agg consists of 2771 rows of interest\n",
    "trip_data.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9d90d9e8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Display the Sample Data\n",
    "trip_data.head().T"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "2a76b312",
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'trip_data' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-11-fee63267b2d7>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0;31m# Defining revenue metrics\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 2\u001b[0;31m \u001b[0mtrip_data\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'NUM_PASSENGERS'\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mceil\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtrip_data\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'AVG_OCCUPANCY_RATE'\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m*\u001b[0m \u001b[0;36m400\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      3\u001b[0m \u001b[0mtrip_data\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'BASE_REVENUE'\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtrip_data\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'NUM_PASSENGERS'\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m*\u001b[0m \u001b[0mtrip_data\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'FARE_PER_PASSENGER'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0mtrip_data\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'LUGGAGE_REVENUE'\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtrip_data\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'NUM_PASSENGERS'\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m*\u001b[0m \u001b[0;36m70\u001b[0m\u001b[0;34m/\u001b[0m\u001b[0;36m2\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0mtrip_data\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'TRIP_REVENUE'\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtrip_data\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'BASE_REVENUE'\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0mtrip_data\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'LUGGAGE_REVENUE'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mNameError\u001b[0m: name 'trip_data' is not defined"
     ]
    }
   ],
   "source": [
    "# Defining revenue metrics\n",
    "trip_data['NUM_PASSENGERS'] = np.ceil(trip_data['AVG_OCCUPANCY_RATE'] * 400)\n",
    "trip_data['BASE_REVENUE'] = trip_data['NUM_PASSENGERS'] * trip_data['FARE_PER_PASSENGER']\n",
    "trip_data['LUGGAGE_REVENUE'] = trip_data['NUM_PASSENGERS'] * 70/2\n",
    "trip_data['TRIP_REVENUE'] = trip_data['BASE_REVENUE'] + trip_data['LUGGAGE_REVENUE']\n",
    "trip_data['TRIP_PROFIT'] = trip_data['TRIP_REVENUE'] - trip_data['TRIP_COST']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "07a6a1de",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Defining Profit Metrics\n",
    "trip_data['TOTAL_PROFIT'] = trip_data['FLIGHT_COUNT'] * trip_data['TRIP_PROFIT']\n",
    "trip_data['TOTAL_COST'] = trip_data['FLIGHT_COUNT'] * trip_data['TRIP_COST']\n",
    "trip_data['TOTAL_REVENUE'] = trip_data['FLIGHT_COUNT'] * trip_data['TRIP_REVENUE']"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a1483f01",
   "metadata": {},
   "source": [
    "#### Outlier Detection"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f2cf6cc7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Detecting outliers present in the Trip Revenue column \n",
    "sns.histplot(trip_data['TRIP_REVENUE'])\n",
    "plt.title(\"Outliers in the Trip Revenue column\")\n",
    "plt.xlabel(\"Trip Revenue in USD\")\n",
    "plt.ylabel(\"Number of Trips\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4212ad9b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Detecting outliers present in the Trip Profit column \n",
    "sns.histplot(trip_data['TRIP_PROFIT'])\n",
    "plt.title(\"Outliers in the Trip Profit column\")\n",
    "plt.xlabel(\"Trip Profit in USD\")\n",
    "plt.ylabel(\"Number of Trips\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6c2206c4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Getting the average statistics at ROUTE-LEVEL\n",
    "trip_data['AVG_PROFIT'] = trip_data['TOTAL_PROFIT']/trip_data['NUM_AIRLINES']\n",
    "trip_data['AVG_NUM_FLIGHTS'] = np.ceil(trip_data['FLIGHT_COUNT']/trip_data['NUM_AIRLINES'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "459e01b8",
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "# Detecting outliers present in the Avg number of flights column \n",
    "sns.histplot(trip_data['AVG_NUM_FLIGHTS'])\n",
    "plt.title(\"Outliers in the Avg Number of Flights column\")\n",
    "plt.xlabel(\"Avg Number of Flights\")\n",
    "plt.ylabel(\"Number of Trips\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "99d72322",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Detecting outliers present in the Avg Trip Profit column \n",
    "sns.histplot(trip_data['AVG_PROFIT'])\n",
    "plt.title(\"Outliers in the Avg Trip Profit column\")\n",
    "plt.xlabel(\"Avg Trip Profit in USD\")\n",
    "plt.ylabel(\"Number of Trips\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7880e2fc",
   "metadata": {},
   "source": [
    "As seen from the plots there are few data points that are present all through the cost axis for the Avg number of flights, Avg Trip Profit,Trip Profit and Trip Revenue. Hence it is not in the best interest to get rid of those outliers for further analysis and evaluation of metrics."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f6ad8339",
   "metadata": {},
   "source": [
    "## Summarizing Insights"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2230d073",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Describing the final merged dataset in a transposed fashion to view all the columns at once\n",
    "trip_data.describe().apply(lambda s: s.apply('{0:.5f}'.format)).T"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8cbe8efe",
   "metadata": {},
   "source": [
    "#### Checking the late % for the above subset of flights"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5923d3fd",
   "metadata": {},
   "outputs": [],
   "source": [
    "# late_pct = flights.groupby('TRIP').agg({'LATE_FEES':['count',np.count_nonzero]}).reset_index()\n",
    "def find_positive_pct(df):\n",
    "    return df[df['LATE_FEES'] > 0].shape[0]/df.shape[0]\n",
    "\n",
    "late_pct = flights.groupby('TRIP').apply(find_positive_pct).reset_index()\n",
    "late_pct.columns = ['TRIP','LATE_PCT']\n",
    "late_pct['ROUND_TRIP'] = late_pct['TRIP'].apply(lambda x: make_key(x))\n",
    "\n",
    "late_data = late_pct.groupby('ROUND_TRIP')['LATE_PCT'].mean().reset_index()\n",
    "late_data.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "02819715",
   "metadata": {},
   "source": [
    "As seen they are the top roundtrips that have highest late fees percentage."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ba81ceda",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Merge the trips data with the late fees data calculated above\n",
    "trip_data = pd.merge(trip_data,\n",
    "                     late_data,\n",
    "                     on = 'ROUND_TRIP',\n",
    "                     how = 'inner')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "750edec4",
   "metadata": {},
   "source": [
    "### Preprocessing for Visualization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9cbee0c5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Splitting he roundtrip as Origin and Destination\n",
    "trip_data['ORIGIN'] = trip_data['ROUND_TRIP'].apply(lambda x: x.split('-')[0])\n",
    "trip_data['DESTINATION'] = trip_data['ROUND_TRIP'].apply(lambda x: x.split('-')[1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1575bf8d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Merging the trips data with US airports subset data on Origin and IATA Code\n",
    "trip_data = pd.merge(trip_data,\n",
    "                            US_airport_subset[['IATA_CODE','COORDINATES']],\n",
    "                            left_on = 'ORIGIN',\n",
    "                            right_on = 'IATA_CODE',\n",
    "                            how = 'inner')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ae600eae",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Dropping the duplicate column and renaming the coordinates column \n",
    "trip_data.drop(['IATA_CODE'], axis = 1, inplace=True)\n",
    "trip_data.rename(columns={'COORDINATES':'SOURCE_COORDINATES'}, inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e34be709",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Merging the trips data with US airports subset data on Destination and IATA Code\n",
    "trip_data = pd.merge(trip_data,\n",
    "                            US_airport_subset[['IATA_CODE','COORDINATES']],\n",
    "                            left_on = 'DESTINATION',\n",
    "                            right_on = 'IATA_CODE',\n",
    "                            how = 'inner')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bf304c8a",
   "metadata": {},
   "outputs": [],
   "source": [
    "trip_data.drop(['IATA_CODE'], axis = 1, inplace=True)\n",
    "trip_data.rename(columns={'COORDINATES':'DESTINATION_COORDINATES'}, inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6e837861",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Splitting the Latitude and Longitude for each cooredinate of the orgin and destination of the trip\n",
    "trip_data['SOURCE_LONG'] = trip_data['SOURCE_COORDINATES'].apply(lambda x: float(x.split(',')[0]))\n",
    "trip_data['SOURCE_LAT'] = trip_data['SOURCE_COORDINATES'].apply(lambda x: float(x.split(',')[1]))\n",
    "trip_data['DESTINATION_LONG'] = trip_data['DESTINATION_COORDINATES'].apply(lambda x: float(x.split(',')[0]))\n",
    "trip_data['DESTINATION_LAT'] = trip_data['DESTINATION_COORDINATES'].apply(lambda x: float(x.split(',')[1]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "eedda293",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Fuction to plot the geographic mapping\n",
    "def plot_flight_path(df, title):\n",
    "    \n",
    "    airport_list = list(set(list(df['ORIGIN'].values) + list(df['DESTINATION'].values)))\n",
    "    airports = US_airport_subset[US_airport_subset['IATA_CODE'].isin(airport_list)][['IATA_CODE','COORDINATES']]\n",
    "    airports['LONG'] = airports['COORDINATES'].apply(lambda x: float(x.split(',')[0]))\n",
    "    airports['LAT'] = airports['COORDINATES'].apply(lambda x: float(x.split(',')[1]))\n",
    "    \n",
    "    fig = go.Figure()\n",
    "\n",
    "    fig.add_trace(go.Scattergeo(\n",
    "        locationmode = 'USA-states',\n",
    "        lon = airports['LONG'],\n",
    "        lat = airports['LAT'],\n",
    "#         hoverinfo = 'text',\n",
    "#         text = 'AIRPORT',\n",
    "        mode = 'markers',\n",
    "        marker = dict(\n",
    "            size = 5,\n",
    "            color = 'rgb(255, 0, 0)',\n",
    "            line = dict(\n",
    "                width = 4,\n",
    "                color = 'rgba(68, 68, 68, 0)'\n",
    "            )\n",
    "        )))\n",
    "\n",
    "    fig.add_trace(go.Scattergeo(\n",
    "        locationmode = 'USA-states',\n",
    "        lon = airports['LONG'],\n",
    "        lat = airports['LAT'],\n",
    "        hoverinfo = 'text',\n",
    "        text = airports['IATA_CODE'],\n",
    "        mode = 'text',\n",
    "        ))\n",
    "\n",
    "    \n",
    "    lons = []\n",
    "    lats = []\n",
    "    import numpy as np\n",
    "\n",
    "    lons = np.empty(3 * len(df))\n",
    "    lons[::3] = df['SOURCE_LONG']\n",
    "    lons[1::3] = df['DESTINATION_LONG']\n",
    "    lons[2::3] = None\n",
    "\n",
    "    lats = np.empty(3 * len(df))\n",
    "    lats[::3] = df['SOURCE_LAT']\n",
    "    lats[1::3] = df['DESTINATION_LAT']\n",
    "    lats[2::3] = None\n",
    "\n",
    "    fig.add_trace(\n",
    "        go.Scattergeo(\n",
    "            locationmode = 'USA-states',\n",
    "            lon = lons,\n",
    "            lat = lats,\n",
    "            mode = 'lines',\n",
    "            line = dict(width = 1,color = 'green'),\n",
    "            opacity = 0.5\n",
    "        )\n",
    "    )\n",
    "\n",
    "    fig.update_layout(\n",
    "        title_text = title,\n",
    "        showlegend = False,\n",
    "        geo = go.layout.Geo(\n",
    "            scope = 'north america',\n",
    "            projection_type = 'azimuthal equal area',\n",
    "            showland = True,\n",
    "            landcolor = 'rgb(243, 243, 243)',\n",
    "            countrycolor = 'rgb(204, 204, 204)',\n",
    "        ),\n",
    "        height=700,\n",
    "    )\n",
    "\n",
    "    fig.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f0272691",
   "metadata": {},
   "source": [
    "### 1. Top 10 Busiest round trips in terms of number of flights in a quarter"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b9cb9051",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Display the 10 most busiest routes trips\n",
    "busiest_routes = trip_data.sort_values(by='FLIGHT_COUNT', ascending=False).head(10)\n",
    "busiest_routes[['ROUND_TRIP','FLIGHT_COUNT','NUM_AIRLINES','AVG_NUM_FLIGHTS']]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0952c196",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plotting the busiest trips on a geographic map\n",
    "plot_flight_path(busiest_routes, 'Top 10 Busiest flight paths')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "607f72a6",
   "metadata": {},
   "source": [
    "### 2. Top 10 Profitable round trips in terms of total profit in a quarter"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c772e1e6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Display the 10 most profitable routes trips\n",
    "profitable_routes = trip_data.sort_values(by='AVG_PROFIT', ascending=False).head(10)\n",
    "profitable_routes['AVG_PROFIT'] = profitable_routes['AVG_PROFIT'].astype('int64') \n",
    "profitable_routes[['ROUND_TRIP','NUM_AIRLINES','AVG_PROFIT']]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0a6e0ae9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plotting the profitable trips on a geographic map\n",
    "plot_flight_path(profitable_routes, 'Top 10 Profitable routes')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4a5ce1dd",
   "metadata": {},
   "source": [
    "#### Summary Cols"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "02064e22",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Dropping any redundant columns to main consistency \n",
    "drop_cols = ['ORIGIN','DESTINATION','SOURCE_COORDINATES','DESTINATION_COORDINATES','SOURCE_LONG',\n",
    "             'SOURCE_LAT','DESTINATION_LONG','DESTINATION_LAT']\n",
    "trip_data[trip_data['ROUND_TRIP'].isin(profitable_routes['ROUND_TRIP'].head(10).values)].drop(drop_cols, axis=1).T"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "491075d6",
   "metadata": {},
   "source": [
    "### 3. Recommendation"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "91814fc5",
   "metadata": {},
   "source": [
    "**The top 10 busiest and top 10 profitable routes as candidate regions for setting up operations. Since punctuality is a big part of the company brand image: \"ON TIME, FOR YOU\", the late time as well on a particular route is factored.**\n",
    "\n",
    "**Also, it is observe that most of the top 10 profitable airlines is because of it being served by just a single airline for that route. This would make a competitor entry difficult. Hence we also factor in the number of airlines served by the route and put a minimum cap of being served by 3 airlines for the recommendation for easier market entry. Hence to achieve differentiation from competitors also to not provoke competition from the top performing airlines.**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3f659c04",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Classing the trips in accordance with the busiest and profitable trips.\n",
    "busy_list = list(busiest_routes['ROUND_TRIP'].head(10).values)\n",
    "profit_list = list(profitable_routes['ROUND_TRIP'].head(10).values)\n",
    "\n",
    "filter_list = list(set(busy_list + profit_list))\n",
    "trips_subset = trip_data[(trip_data['ROUND_TRIP'].isin(filter_list)) &\n",
    "                         (trip_data['NUM_AIRLINES'] >= 3)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "82fd9fdb",
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "# Sorting the trips with the most charged late percentage\n",
    "trips_subset.sort_values(by='LATE_PCT').head(5).T"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "558f0740",
   "metadata": {},
   "source": [
    "#### Recommended round trips"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "95e55830",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Hence these are the recommended trips based on the late percentage as punctualality plays a key role for the brand\n",
    "recommended_trips = trips_subset.sort_values(by='LATE_PCT').head(5)['ROUND_TRIP'].values\n",
    "recommended_trip = trips_subset[trips_subset['ROUND_TRIP'].isin(recommended_trips)]\n",
    "recommended_trip.drop(['TOTAL_PROFIT','TOTAL_COST','TOTAL_REVENUE'], axis = 1, inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ded37f2b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Display the Origin and Destination for the round trip with the Avg profit made by each\n",
    "recommended_trip['ORIGIN'] = recommended_trip['ROUND_TRIP'].apply(lambda x: x.split('-')[0])\n",
    "recommended_trip['DESTINATION'] = recommended_trip['ROUND_TRIP'].apply(lambda x: x.split('-')[1])\n",
    "recommended_trip['AVG_PROFIT'] = recommended_trip['AVG_PROFIT'].astype('int64') \n",
    "recommended_trip[['ORIGIN','DESTINATION','AVG_PROFIT']]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0a74b94d",
   "metadata": {},
   "source": [
    "### 4. Number of flights to breakeven"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c10d1643",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Calcuting the number of flights and quarters to breakeven for each recommended route\n",
    "recommended_trip['FLIGHTS_TO_BREAKEVEN'] = np.ceil(90*1000000/recommended_trip['TRIP_REVENUE'])\n",
    "recommended_trip['AVG_FLIGHTS'] = round(recommended_trip['FLIGHT_COUNT']/recommended_trip['NUM_AIRLINES'],0)\n",
    "recommended_trip['NUM_QUARTER_TO_BREAKEVEN'] = np.ceil(recommended_trip['FLIGHTS_TO_BREAKEVEN']/recommended_trip['AVG_FLIGHTS'])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "da191d10",
   "metadata": {},
   "source": [
    "#### Time taken to breakeven"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3724046c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Displaying the breakeven details for each trip\n",
    "recommended_trip[['ROUND_TRIP','FLIGHTS_TO_BREAKEVEN','AVG_FLIGHTS','NUM_QUARTER_TO_BREAKEVEN','FARE_PER_PASSENGER']]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b614da5a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plotting the geographic mapping of the recommended trips\n",
    "plot_flight_path(recommended_trip, 'Recommended Trips')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cc8f3638",
   "metadata": {},
   "source": [
    "### 5. KPI's to track for future"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "acd1f7f0",
   "metadata": {},
   "source": [
    "The airlines industry is made up of companies whose primary business activities are focused on air transportation for passengers and cargo and includes major network carriers, regional carriers and low-cost carriers. Below is current consensus forecast data on capacity metrics, such as available seat miles (ASMs); utilization metrics, such as load factor; and other airline KPIs used to look for an investment edge, including revenue passenger miles (RPMs), passenger revenue per mile flown (PRASM), total costs and expenses per ASM (CASM) and fuel expenses. These key airline metrics aid market participants in identifying airline trends and future performance of airline companies, including Southwest Airlines, Air Canada, Air China and more.\n",
    "\n",
    "From the above data analysis the basic understanding of the airline market expectations whether at a company or an industry level should be establsihed. \n",
    "\n",
    "There are two Metrics that can be looked into for further use.\n",
    "  \n",
    "  1. **PRASM**: Passenger Revenue per Available Seat Mile is calculated by dividing passenger revenue by available seat miles. PRASM is also equivalent to the product of load factor and passenger yield. \n",
    "  2. **TRASM**: Total Revenue per Available Seat Mile is calculated by a sum of total revenue, ancillary revenue and cargo revenue\n",
    "Both these metrics have few common KPI's which are catagorized into:\n",
    "  1. Volume factors\n",
    "  2. Price Factors  \n",
    "  3. Utilizations facotrs \n",
    "  \n",
    "These can be further broken down into few other key factors:\n",
    "  - **PLF**:Passenger Load Factor(%) is a measure of number revenue passengers miles(or km) expressed as percentage of ASM.\n",
    "  - **Revenue Passenger Miles(RPM)** is a measure of volume and is calculated by number of passengers multiplied by the miles of a flight.\n",
    "  - **Passesger(ASM) Available Seat Miles** is a measure of airline capacity and is calculated by taking the number of seats available multiplied by distance. \n",
    "  - **Traffic** = Capacity * PLF \n",
    "  - **Revenue Per Available Seat Mile(RASM)** is calulated by dividing passenger revenue by avaliable seat miles.\n",
    "  - **Cost per Available Seat Mile(CASM)** is a measure of effiency and is calculated by taking operating expenses and dividing by ASM.\n",
    "  - **CASM-Ex** is calculated by operating expenses dividing by ASM and then subtracting the cost of fuel.\n",
    "  - **Yield** is defined by Passenger Yield Cents and Cargo Yield Cents\n",
    "  - **Revenue** = Traffic * Yield\n",
    "  - **Operating Income** = Revenue - Operating Expenses(labor compensation, fuel consumption, airport landing fees, airport maintainence)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f7d243ef",
   "metadata": {},
   "source": [
    "### What's next?\n",
    "\n",
    "In the analysis, we have aggregated the data at a **ROUND TRIP** level. That is all flights flying from a particular ORIGIN to DESTINATION and back, irrespective of the carrier, have been considered equal. This may introduce **SIMPSONS PARADOX** in the form of **AGGREGATION BIAS** and analyzing the above information at a carrier level may bring out additional insights.  \n",
    "  \n",
    "Further, another factor that we have ignored in this analysis is **TIME**. We have considered all the flights at a **QUARTER** level. We can further break this down into **MONTH, WEEK and DAY level** data to obtain additional insights.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "93a8548c",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Index",
   "title_sidebar": "Index",
   "toc_cell": true,
   "toc_position": {},
   "toc_section_display": true,
   "toc_window_display": true
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
